{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Define the LLM\n",
        "os.environ[\"GROQ_API_KEY\"] = \"ENTER API KEY\"\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=200\n",
        ")"
      ],
      "metadata": {
        "id": "od6fPHcfkkyR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simply call invoke method on the llm and pass in the input question.\n",
        "\n",
        "#:#Step 2 - Invoke llm with a fixed text input\n",
        "\n",
        "response = llm.invoke(\"Tell me one fact about space\", temperature=0.7)\n",
        "print(\"Scenario 1 Response - >\")\n",
        "print(response.pretty_print())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnnx-5jC_ulQ",
        "outputId": "fb5c35e7-7a0b-4518-f711-ac73db53655f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 1 Response - >\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "One fascinating fact about space is that there is a giant storm on Jupiter called the Great Red Spot, which has been continuously raging for at least 187 years and possibly much longer.\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template to structure user inputs. Template allows us to collect inputs at runtime.\n",
        "\n",
        "#:#Step 3 - Use String Prompt to accept text input. Here we create a template and\n",
        "#declare a input variable {user_input} and {city}\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "Human: {user_input} {city}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "11XxkmGd_unk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#:#Step 4 - Here we create a Prompt using the template\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"user_input\", \"city\"], template=template)"
      ],
      "metadata": {
        "id": "sKrtGmDY_upy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#:#Step 5 - Here we get a prompt value and print it.\n",
        "\n",
        "prompt_val = prompt.invoke({\"user_input\":\"Tell us in an Pirate tone about\", \"city\":\"Las Vegas\"})\n",
        "print(\"Prompt String is ->\")\n",
        "print(prompt_val.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BVwwIEW_usD",
        "outputId": "fa68c772-43ea-45c8-c998-737b126cd743"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt String is ->\n",
            "You are a chatbot having a conversation with a human.\n",
            "Human: Tell us in an Pirate tone about Las Vegas\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the prompt template and the OCI Generative AI model using the pipe operator. In this setup, the invoke method processes the input through the defined chain, producing the desired output.\n",
        "\n",
        "#:#Step 6 - here we declare a chain that begins with a prompt, next llm\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "L1huLfmhAKjG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#:#Step 7 - Next we invoke a chain, get response and print it.\n",
        "\n",
        "response = chain.invoke({\"user_input\":\"Tell us in a priate tone about\", \"city\":\"New York\"})\n",
        "\n",
        "print(\"Scenario 2 Response - >\")\n",
        "print(response.pretty_print())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNw7AQp2AKlW",
        "outputId": "68f91765-cd71-4d7d-919c-771f7e23dba8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 2 Response - >\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "(in a charming, pirate-like tone) Ahoy, matey! Ye be wantin' to know about the grand city o' New York, eh? Well, settle yerself down with a pint o' grog and listen close, for I be tellin' ye tales o' the Big Apple.\n",
            "\n",
            "New York, the city that never sleeps, be a place o' wonder and magic. 'Tis a melting pot o' cultures, where the streets be filled with the sounds o' laughter, music, and the clinkin' o' glasses. From the bright lights o' Times Square to the peaceful greenery o' Central Park, this city be a treasure trove o' excitement and adventure.\n",
            "\n",
            "Ye can stroll along the mighty Hudson River, takin' in the sights o' the Statue o' Liberty and the Brooklyn Bridge. Or, ye can explore the many museums and art galleries, where ye can see the works o' the masters and learn about the\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chat prompt template.\n",
        "\n",
        "#:#Step 8 - Use Chat Message Prompt to accept text input. Here we create a chat template and\n",
        "#use HumanMessage and SystemMessage\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a chatbot that explains in steps.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "C6ycIJSvAKn_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#:#Step 9 - Here we get a prompt value and print it.\n",
        "\n",
        "prompt_value = chat_prompt.invoke({\"input\":\"What is microbiology?\"})\n",
        "print(prompt_value)\n",
        "# messages=[SystemMessage(content='You are a chatbot that explains in steps.'), HumanMessage(content='What is microbiology?')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-IA8YuFAKqQ",
        "outputId": "2dcf30a2-928e-4f18-e158-b3f049d56561"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a chatbot that explains in steps.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is microbiology?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the chat prompt template and the OCI Generative AI model using the pipe operator. In this setup, the invoke method processes the input through the defined chain, producing the desired output.\n",
        "\n",
        "#:#Step 10 - create another chain, get a response and print it.\n",
        "\n",
        "chain1 = chat_prompt | llm\n",
        "response = chain1.invoke({\"input\": \"What's the New York culture like?\"})\n",
        "\n",
        "print(\"Scenario 3 Response - >\")\n",
        "print(response.pretty_print())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUbn4dj0AKsi",
        "outputId": "b823b9df-e6e7-465f-f618-7ec87c080eaa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 3 Response - >\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Exploring New York culture can be a fascinating topic. Here's a step-by-step breakdown:\n",
            "\n",
            "**Step 1: Diversity and Melting Pot**\n",
            "New York City is a melting pot of cultures, with over 8.4 million people from different ethnic backgrounds, speaking over 800 languages. This diversity is reflected in the food, music, art, and lifestyle of the city.\n",
            "\n",
            "**Step 2: Fast-paced and Dynamic**\n",
            "New York is known for its fast-paced and dynamic environment. People are often in a hurry, and the city never sleeps. This energy is reflected in the 24/7 lifestyle, with restaurants, shops, and entertainment options available around the clock.\n",
            "\n",
            "**Step 3: Food Culture**\n",
            "New York's food culture is a reflection of its diversity. You can find authentic Chinese, Italian, Mexican, Indian, and many other cuisines in the city. Some popular New York dishes include pizza, bagels, pastrami sandwiches, and black and white cookies.\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessageGraph\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# 1. Define the checkpointer (memory)\n",
        "# This is the same as your code.\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# 2. Define the graph\n",
        "# MessageGraph is the simplest way to build a stateful chatbot\n",
        "graph_builder = MessageGraph()\n",
        "\n",
        "# 3. Define the function that calls your LLM\n",
        "# This function will be a \"node\" in the graph\n",
        "def llm_node(state):\n",
        "    # 'state' is the list of messages from memory\n",
        "    # We pass this entire history to the llm\n",
        "    return llm.invoke(state)\n",
        "\n",
        "# 4. Add the node to the graph\n",
        "graph_builder.add_node(\"llm\", llm_node)\n",
        "\n",
        "# 5. Define the graph's start and end points\n",
        "graph_builder.set_entry_point(\"llm\")\n",
        "graph_builder.set_finish_point(\"llm\")\n",
        "\n",
        "# 6. Compile the graph into the final \"agent\"\n",
        "# THIS is the step that connects the agent to the memory\n",
        "agent = graph_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"Chat agent with memory is compiled and ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmlq_ggNAKub",
        "outputId": "188bd757-4361-4e7a-9b1a-040dc2c7aac0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat agent with memory is compiled and ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-916435198.py:10: LangGraphDeprecatedSinceV10: MessageGraph is deprecated in LangGraph v1.0.0, to be removed in v2.0.0. Please use StateGraph with a `messages` key instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  graph_builder = MessageGraph()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#:#Step 12 - Here we ask our first question and print response and memory contents\n",
        "\n",
        "# Define the config for the conversation thread\n",
        "conversation_id = \"my-first-graph-chat\"\n",
        "config = {\"configurable\": {\"thread_id\": conversation_id}}\n",
        "\n",
        "# The input is a list of messages.\n",
        "msg1 = [(\"user\", \"Hi! My name is Ayush.\")]\n",
        "\n",
        "print(\"--- Turn 1 ---\")\n",
        "# 1. Invoke the agent\n",
        "response_messages_1 = agent.invoke(msg1, config=config)\n",
        "\n",
        "# Get the last message (the AI's reply)\n",
        "ai_response_1 = response_messages_1[-1]\n",
        "print(f\"AI Response: {ai_response_1.content}\")\n",
        "\n",
        "# 2. Get the full history from the checkpointer's memory\n",
        "memory_state_1 = checkpointer.get(config)\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# The 'memory_state_1' variable IS the list of messages\n",
        "print(f\"\\nMemory Contents: {memory_state_1['channel_values']['__root__']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SORjvk8ngAE5",
        "outputId": "0af6bc82-2013-4b56-a93e-b8be16ce09ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Turn 1 ---\n",
            "AI Response: Hello Ayush, it's nice to meet you. Is there something I can help you with or would you like to chat?\n",
            "\n",
            "Memory Contents: [HumanMessage(content='Hi! My name is Ayush.', additional_kwargs={}, response_metadata={}, id='5e89a686-537e-4b3a-823b-42e242c271b3'), AIMessage(content=\"Hello Ayush, it's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 43, 'total_tokens': 70, 'completion_time': 0.024533283, 'prompt_time': 0.002030763, 'queue_time': 0.022542787, 'total_time': 0.026564046}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6b5c123dd9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--4805ec3c-deb9-400e-873c-060a89621d6a-0', usage_metadata={'input_tokens': 43, 'output_tokens': 27, 'total_tokens': 70})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#:#Step 13 - Here we ask our followup question and print response and memory contents\n",
        "\n",
        "# The config is the same. We just send the new message.\n",
        "msg2 = [(\"user\", \"Can you tell what is my name?\")]\n",
        "\n",
        "print(\"\\n--- Turn 2 ---\")\n",
        "# 1. Invoke the agent again\n",
        "response_messages_2 = agent.invoke(msg2, config=config) # Use the same config\n",
        "\n",
        "# Get the last message (the AI's reply)\n",
        "ai_response_2 = response_messages_2[-1]\n",
        "print(f\"AI Response: {ai_response_2.content}\")\n",
        "\n",
        "# 2. Get the updated history from the checkpointer's memory\n",
        "memory_state_2 = checkpointer.get(config)\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# The 'memory_state_2' variable IS the list of messages\n",
        "print(f\"\\nMemory Contents: {memory_state_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j15HE3m4gAHm",
        "outputId": "4fb2e45d-0d8d-4d5b-fe33-2f387cf80577"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 2 ---\n",
            "AI Response: Your name is Ayush.\n",
            "\n",
            "Memory Contents: {'v': 4, 'ts': '2025-11-04T09:19:33.089098+00:00', 'id': '1f0b95f5-fed1-6ab1-8004-df7d62b8ac6a', 'channel_versions': {'__start__': '00000000000000000000000000000005.0.4501086194323214', '__root__': '00000000000000000000000000000006.0.8149478015795615', 'branch:to:llm': '00000000000000000000000000000006.0.8149478015795615'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5859061446560523'}, 'llm': {'branch:to:llm': '00000000000000000000000000000005.0.4501086194323214'}}, 'updated_channels': ['__root__'], 'channel_values': {'__root__': [HumanMessage(content='Hi! My name is Ayush.', additional_kwargs={}, response_metadata={}, id='5e89a686-537e-4b3a-823b-42e242c271b3'), AIMessage(content=\"Hello Ayush, it's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 43, 'total_tokens': 70, 'completion_time': 0.024533283, 'prompt_time': 0.002030763, 'queue_time': 0.022542787, 'total_time': 0.026564046}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6b5c123dd9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--4805ec3c-deb9-400e-873c-060a89621d6a-0', usage_metadata={'input_tokens': 43, 'output_tokens': 27, 'total_tokens': 70}), HumanMessage(content='Can you tell what is my name?', additional_kwargs={}, response_metadata={}, id='3083e59e-62ae-4990-97cb-e73cfdd8a333'), AIMessage(content='Your name is Ayush.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 87, 'total_tokens': 94, 'completion_time': 0.009283918, 'prompt_time': 0.004703753, 'queue_time': 0.021321787, 'total_time': 0.013987671}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6b5c123dd9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--4f93f34b-ec44-4a72-86b9-ff4762765904-0', usage_metadata={'input_tokens': 87, 'output_tokens': 7, 'total_tokens': 94})]}}\n"
          ]
        }
      ]
    }
  ]
}